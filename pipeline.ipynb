{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Introduction**\n",
    "\n",
    "## **Objective**\n",
    "This notebook explores how different transformer architecturesâ€”encoder-only, decoder-only, and encoder-decoderâ€”are applied to various NLP tasks. Through hands-on examples, we will compare their performance and limitations to better understand their strengths and use cases."
   ],
   "id": "614a2794ab4ba9a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Overview of Transformer Architectures**\n",
    "\n",
    "- **Encoder-Only Models (e.g., BERT)**:\n",
    "  - Specialized for understanding and extracting meaning from input text.\n",
    "  - Commonly used for tasks like text classification, sentiment analysis, and feature extraction.\n",
    "\n",
    "- **Decoder-Only Models (e.g., GPT-2)**:\n",
    "  - Designed for generating coherent sequences of text.\n",
    "  - Best suited for tasks like text generation and auto-completion.\n",
    "\n",
    "- **Encoder-Decoder Models (e.g., T5, BART)**:\n",
    "  - Built for sequence-to-sequence tasks like summarization and translation.\n",
    "  - They first encode the input text into meaningful representations, then decode it into the desired output."
   ],
   "id": "15c6dc63e893fae3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Transformers are flexible, but no single architecture is optimal for all tasks.\n",
    "\n",
    "| **Architecture**    | **Examples**            | **Main Applications**                   |\n",
    "|----------------------|-------------------------|-----------------------------------------|\n",
    "| Encoder-Only         | BERT, RoBERTa          | Sentiment analysis, text classification |\n",
    "| Decoder-Only         | GPT-2, GPT-4o, Llama3.x           | Text generation, creative writing       |\n",
    "| Encoder-Decoder      | T5, BART        | Translation, summarization |\n"
   ],
   "id": "9d92ffe28967a56c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Task 1: Sentiment Analysis with a pre-trained version of BERT**",
   "id": "104423cf4a9ed7e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T14:57:55.939548Z",
     "start_time": "2025-01-02T14:57:52.508833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the required libraries\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Load the pre-trained sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Step 2: Define input texts for analysis\n",
    "print(\"Defining input texts...\")\n",
    "texts = [\n",
    "    \"I love using this library! It's so intuitive and powerful.\",\n",
    "    \"This is the worst app I've ever used. Completely useless.\",\n",
    "    \"It's okay, does the job but nothing extraordinary.\",\n",
    "    \"Absolutely fantastic! Highly recommended to everyone.\",\n",
    "    \"Terrible experience, would never use it again.\"\n",
    "]\n",
    "print(f\"Input Texts: {texts}\\n\")\n",
    "\n",
    "# Step 3: Perform sentiment analysis\n",
    "print(\"Performing sentiment analysis...\")\n",
    "results = sentiment_analyzer(texts)\n",
    "\n",
    "# Step 4: Map labels to human-readable sentiments\n",
    "label_mapping = {\n",
    "    \"LABEL_0\": \"negative\",\n",
    "    \"LABEL_1\": \"neutral\",\n",
    "    \"LABEL_2\": \"positive\"\n",
    "}\n",
    "\n",
    "# Step 5: Display results with human-readable labels\n",
    "print(\"\\n--- Sentiment Analysis Results ---\\n\")\n",
    "for text, result in zip(texts, results):\n",
    "    sentiment = label_mapping.get(result['label'], \"unknown\")  # Map label to sentiment\n",
    "    score = result['score']\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"  Sentiment: {sentiment} | Confidence: {score:.2f}\\n\")\n"
   ],
   "id": "7cc65fc4345487a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/PycharmProjects/transformers-pipeline/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining input texts...\n",
      "Input Texts: [\"I love using this library! It's so intuitive and powerful.\", \"This is the worst app I've ever used. Completely useless.\", \"It's okay, does the job but nothing extraordinary.\", 'Absolutely fantastic! Highly recommended to everyone.', 'Terrible experience, would never use it again.']\n",
      "\n",
      "Performing sentiment analysis...\n",
      "\n",
      "--- Sentiment Analysis Results ---\n",
      "\n",
      "Text: I love using this library! It's so intuitive and powerful.\n",
      "  Sentiment: positive | Confidence: 0.99\n",
      "\n",
      "Text: This is the worst app I've ever used. Completely useless.\n",
      "  Sentiment: negative | Confidence: 0.98\n",
      "\n",
      "Text: It's okay, does the job but nothing extraordinary.\n",
      "  Sentiment: neutral | Confidence: 0.58\n",
      "\n",
      "Text: Absolutely fantastic! Highly recommended to everyone.\n",
      "  Sentiment: positive | Confidence: 0.98\n",
      "\n",
      "Text: Terrible experience, would never use it again.\n",
      "  Sentiment: negative | Confidence: 0.98\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Task 2: Text Generation with GPT-2**",
   "id": "5d859c2eca47609b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:09:20.223963Z",
     "start_time": "2025-01-02T15:09:16.835856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Input text (prompt for generation)\n",
    "input_text = \"Please translate 'Artificial Intelligence' into German.\"\n",
    "\n",
    "# Encode the input text and convert it to tensor format\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the output text based on the input\n",
    "outputs = model.generate(inputs, max_length=160, num_return_sequences=1, no_repeat_ngram_size=2, temperature=0.7)\n",
    "\n",
    "# Decode the generated token IDs back to human-readable text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(f\"Generated Text: {generated_text}\")"
   ],
   "id": "c42f5510ab0692a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/PycharmProjects/transformers-pipeline/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Please translate 'Artificial Intelligence' into German.\n",
      "\n",
      "The German language is a very important part of the world. It is the language of many cultures. The German people are very good at it. They are the best at reading and writing. In fact, they are so good that they can write in English. But they don't understand English very well. So they write it in German, and then they translate it into English, which is very difficult. And then, of course, the English language has a lot of problems. For example, it is not very easy to understand. You have to learn how to read. There are many problems with it, but it's very hard to translate. I think that's why the German is so important. Because it has so many languages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pipeline of Tasks with different Transformer Architectures\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "This pipeline demonstrates how different Transformer models can be combined to accomplish a series of NLP tasks in a sequential manner. Each step showcases a unique capability of Transformer architectures, highlighting their strengths and versatility.\n",
    "\n",
    "#### Key Steps:\n",
    "1. **Fill-Mask Task**: Use a masked language model (BERT) to predict the missing word in a sentence.\n",
    "2. **Text Generation**: Generate a continuation of the unmasked sentence using a decoder-only model (GPT-2).\n",
    "3. **Sentiment Analysis**: Analyze the sentiment of the generated text using a text classification model.\n",
    "4. **Summarization**: Summarize the generated text using an encoder-decoder model (BART).\n",
    "5. **Question Generation**: Generate a meaningful question based on the summary using an external language model.\n",
    "6. **Question Answering**: Answer the generated question using a pre-trained question-answering model (BERT fine-tuned on SQuAD).\n",
    "\n",
    "This pipeline illustrates how Transformer models complement each other to solve complex tasks, emphasizing the interplay between different architectures like encoder-only, decoder-only, and encoder-decoder models.\n"
   ],
   "id": "98876123966ddef5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 0: Setup and Initialization\n",
    "\n",
    "Before diving into the pipeline tasks, we need to set up the environment and initialize the required libraries and settings. This includes:\n",
    "\n",
    "1. **Importing Necessary Libraries**:\n",
    "   - `subprocess`: To interact with external tools (e.g., Ollama for advanced question generation).\n",
    "   - `os`: For managing environment settings.\n",
    "   - `transformers.pipeline`: The Hugging Face pipeline API to access pre-trained models for various NLP tasks.\n",
    "\n",
    "2. **Disabling Tokenizer Parallelism**:\n",
    "   - To avoid potential warnings or conflicts during execution, we explicitly disable tokenizer parallelism."
   ],
   "id": "18492aef7e626a64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:32:24.642482Z",
     "start_time": "2025-01-02T15:32:24.636686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import required libraries and disable tokenizer parallelism\n",
    "import subprocess\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ],
   "id": "52f545b301e0ed01",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1: Fill-Mask Task\n",
    "The fill-mask task uses a masked language model (BERT) to predict the missing word in a sentence.\n",
    "- **Input**: A sentence with a masked word (`[MASK]`).\n",
    "- **Output**: The sentence with the mask replaced by the most likely word.\n"
   ],
   "id": "806304c7be14de38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:19:42.017149Z",
     "start_time": "2025-01-02T15:19:40.310234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Step 1: Performing fill-mask task...\")\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "masked_sentence = \"Renewable energy reduces [MASK].\"\n",
    "fill_results = mask_filler(masked_sentence, top_k=1)\n",
    "unmasked_sentence = fill_results[0]['sequence']\n",
    "print(f\"Input Sentence: {masked_sentence}\")\n",
    "print(f\"Unmasked Sentence: {unmasked_sentence}\\n\")\n"
   ],
   "id": "77a4af60995f1e89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Performing fill-mask task...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: Renewable energy reduces [MASK].\n",
      "Unmasked Sentence: renewable energy reduces costs.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Text Generation\n",
    "Using a decoder-only model (GPT-2) to generate text based on the unmasked sentence.\n",
    "- **Input**: The output of the fill-mask task.\n",
    "- **Output**: A generated continuation of the sentence.\n"
   ],
   "id": "c221c4f63e1bfaff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:32:59.906020Z",
     "start_time": "2025-01-02T15:32:54.796617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Step 2: Generating text with GPT-2...\")\n",
    "text_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generated_texts = text_generator(unmasked_sentence, max_length=100, num_return_sequences=1, truncation=True)\n",
    "generated_text = generated_texts[0]['generated_text']\n",
    "print(f\"Generated Text: {generated_text}\\n\")\n"
   ],
   "id": "5fc3c14c11a58bb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Generating text with GPT-2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: renewable energy reduces costs. So far, however, efforts have been in vain to get nuclear power plants to meet the energy needs of the next generation.\n",
      "\n",
      "The U.S. Energy Department, after years of neglect, began to act and move forward with a policy aimed at reducing greenhouse gas emissions associated with coal-fired power plants on Wednesday.\n",
      "\n",
      "The policy, known as Energy Policy Reform Act of 2015 (EPRA), lays out clear rules for government and utilities to establish a\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3: Sentiment Analysis\n",
    "\n",
    "The sentiment analysis task evaluates the emotional tone of the generated text. This step uses a text classification model to label the text as **positive**, **negative**, or **neutral**.\n",
    "\n",
    "- **Input**: The generated text from Step 2.\n",
    "- **Output**: Sentiment label and confidence score."
   ],
   "id": "a1cc7b977953b86b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:39:55.470857Z",
     "start_time": "2025-01-02T15:39:54.454690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Step 3: Analyzing sentiment...\")\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sentiment_results = sentiment_analyzer(generated_text)\n",
    "sentiment = sentiment_results[0]['label']\n",
    "confidence = sentiment_results[0]['score']\n",
    "print(f\"Sentiment: {sentiment} | Confidence: {confidence:.2f}\\n\")"
   ],
   "id": "886cc91827cd27d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Analyzing sentiment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: POSITIVE | Confidence: 0.93\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 4: Summarization\n",
    "\n",
    "In this step, the generated text is summarized using an encoder-decoder model. Summarization compresses the information while retaining the key ideas.\n",
    "\n",
    "- **Input**: The generated text from Step 2.\n",
    "- **Output**: A concise summary of the input text."
   ],
   "id": "1f24a2bb10c0203f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:41:46.658293Z",
     "start_time": "2025-01-02T15:41:37.968536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Step 4: Summarizing the text...\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=-1)  # Force CPU\n",
    "\n",
    "try:\n",
    "    summary = summarizer(generated_text, max_length=100, min_length=10, do_sample=False)\n",
    "    summary_text = summary[0]['summary_text']\n",
    "    print(f\"Summary: {summary_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Summarization failed: {e}\")\n",
    "    summary_text = \"No summary available.\""
   ],
   "id": "8c0d07e8203404b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Summarizing the text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: U.S. Energy Department moves forward with policy aimed at reducing greenhouse gas emissions associated with coal-fired power plants. Energy Policy Reform Act of 2015 lays out clear rules for government and utilities to establish a renewable energy program.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5: Question Generation\n",
    "\n",
    "Based on the summary, a meaningful question is generated. This task leverages an external language model (like LLaMA) to create a question that aligns with the context of the summary.\n",
    "\n",
    "- **Input**: Summary text from Step 4.\n",
    "- **Output**: A generated question based on the summary."
   ],
   "id": "2c7107daf3003db0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:43:03.049373Z",
     "start_time": "2025-01-02T15:42:56.668896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_question_with_instruction(summary_text):\n",
    "    print(\"Generating a question based on the summary...\")\n",
    "    instruction = f\"Generate a simple question based on the following summary:\\n\\n{summary_text}\\n\\nQuestion:\"\n",
    "    try:\n",
    "        # Using a subprocess to call Ollama CLI with specific instruction\n",
    "        command = [\"ollama\", \"run\", \"llama3.2\"]\n",
    "        result = subprocess.run(command, input=instruction, text=True, capture_output=True, check=True)\n",
    "        question = result.stdout.strip()\n",
    "        print(f\"Generated Question: {question}\")\n",
    "        return question\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error using Ollama: {e}\")\n",
    "        return \"What are the key points mentioned in the summary?\"  # Fallback question\n",
    "\n",
    "question = generate_question_with_instruction(summary_text)"
   ],
   "id": "ea5eec9ea16e5db5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a question based on the summary...\n",
      "Generated Question: What is the main goal of the U.S. Energy Department's new policy regarding coal-fired power plants?\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 6: Question Answering\n",
    "\n",
    "In the final step, a question-answering model (fine-tuned on SQuAD) is used to answer the generated question using the summary as context.\n",
    "\n",
    "- **Input**:\n",
    "  - Generated question from Step 5.\n",
    "  - Summary text from Step 4 as context.\n",
    "- **Output**: Answer to the question."
   ],
   "id": "8ad1c56a302d4e57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-02T15:44:07.514933Z",
     "start_time": "2025-01-02T15:44:03.981103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Step 6: Question answering task...\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "context = summary_text  # Using the summary as context for QA\n",
    "\n",
    "qa_result = qa_pipeline(question=question, context=context)\n",
    "answer = qa_result['answer']\n",
    "print(f\"Answer: {answer}\\n\")"
   ],
   "id": "e86aee1d0f6350c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Question answering task...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: reducing greenhouse gas emissions\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Conclusion:\n",
    "This pipeline demonstrates the synergy between different Transformer architectures:\n",
    "- **Encoder-only models** excel at understanding and classifying text.\n",
    "- **Decoder-only models** generate creative and coherent sequences.\n",
    "- **Encoder-decoder models** specialize in sequence-to-sequence tasks like summarization.\n",
    "\n",
    "By combining these models, we unlock the full potential of Transformers for complex, multi-step NLP workflows."
   ],
   "id": "8cf700e9eef23f9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
